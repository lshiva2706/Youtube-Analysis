{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lnMLgnvpYMc8"
   },
   "outputs": [],
   "source": [
    "dir = '/content/drive/My Drive/DA_Project/YT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EldWrbU1YfG2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lsandh3\\AppData\\Local\\Temp\\ipykernel_19540\\4051890243.py:12: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML,display\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections,os,json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from IPython.core.display import HTML,display\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=RuntimeWarning)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "eS6hCWO_Yg4a"
   },
   "outputs": [],
   "source": [
    "def set_css():\n",
    "  return display(HTML(\"\"\"<style>\n",
    "  #output-body {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "    background-color:white;\n",
    "    color:black;\n",
    "    }\n",
    "  .output_text pre{\n",
    "    margin-right : 70px;\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "    /* font-size: 10px; */\n",
    "  }\n",
    "\n",
    "}  </style>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oExDVsQYmta"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5_yWwrODSFI"
   },
   "source": [
    "#Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6vZNRi91vMs"
   },
   "source": [
    "##Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Z6p70r9Rot8Q"
   },
   "outputs": [],
   "source": [
    "def process_date_attributes(data):\n",
    "  data[\"publish_time\"] = pd.to_datetime(data[\"publish_time\"])\n",
    "  data[\"trending_date\"] = pd.to_datetime(data[\"trending_date\"],format=\"%y.%d.%m\")\n",
    "  data[\"trend_year\"]=data[\"trending_date\"].apply(lambda time:time.year)\n",
    "  data[\"trend_date\"] = data[\"trending_date\"].apply(lambda time: time.day)\n",
    "  data[\"trend_month\"]=data[\"trending_date\"].apply(lambda time:time.month)\n",
    "  data[\"publish_year\"]=data[\"publish_time\"].apply(lambda time:time.year)\n",
    "  data[\"publish_month\"]=data[\"publish_time\"].apply(lambda time:time.month)\n",
    "  data[\"publish_day\"]=data[\"publish_time\"].apply(lambda time:time.day)\n",
    "  data[\"publish_weekday\"]=data[\"publish_time\"].apply(lambda time:time.dayofweek)\n",
    "  data[\"publish_hour\"]=data[\"publish_time\"].apply(lambda time:time.hour)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "m_VzmTLBBdn2"
   },
   "outputs": [],
   "source": [
    "def match_categories(x,dictionary): #function to get the categories\n",
    "    try:\n",
    "        return dictionary[str(x)]\n",
    "    except:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-OkorSq10s1"
   },
   "source": [
    "##Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3mpQIOPz10Ws"
   },
   "outputs": [],
   "source": [
    "def data_cleaning(data):\n",
    "  if data['description'].isnull().sum()>0:\n",
    "    data['description'] = data['description'].replace(np.nan,\"\") #Replacing the NaN values in the \"Description\" column with an empty string\n",
    "  data = data.sort_values('video_id').drop_duplicates(subset=['trending_date', 'title'], keep='last')\n",
    "  data = data.drop(columns=[\"publish_time\",\"category_id\"],axis=1)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-bO_ZZl5wxq"
   },
   "source": [
    "##Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "bNTt_yKxbf1O"
   },
   "outputs": [],
   "source": [
    "def load_data(file_path): \n",
    "    ch_file=file_path\n",
    "    Country_code = ch_file[0:2]\n",
    "    path=os.path.join(dir,ch_file)\n",
    "    input_df=pd.read_csv(path,encoding='utf-8')\n",
    "    title_dict={}\n",
    "    json_path=os.path.join(dir,(Country_code+str('_category_id.json')))\n",
    "    json_input_df=pd.read_json(json_path)\n",
    "    input_df['region']=Country_code\n",
    "    for dict_a in json_input_df['items']:\n",
    "        title_dict[dict_a['id']]=dict_a['snippet']['title'] \n",
    "    input_df = process_date_attributes(input_df) #Calling the function to process the date columns.\n",
    "    input_df['category']=input_df['category_id'].apply(match_categories,args=(title_dict,))\n",
    "    input_df = data_cleaning(input_df)\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "vYhLfsK1fMDy",
    "outputId": "bb5fa477-6dfc-47e3-e206-0c99b7cb01df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  #output-body {\n",
       "    display: flex;\n",
       "    align-items: center;\n",
       "    justify-content: center;\n",
       "    background-color:white;\n",
       "    color:black;\n",
       "    }\n",
       "  .output_text pre{\n",
       "    margin-right : 70px;\n",
       "    color: black;\n",
       "    font-weight: bold;\n",
       "    /* font-size: 10px; */\n",
       "  }\n",
       "\n",
       "}  </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m           \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------Created\u001b[39m\u001b[38;5;124m\"\u001b[39m,paths[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed all available Datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m t_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m tdf \u001b[38;5;241m=\u001b[39m t_df\n\u001b[0;32m     20\u001b[0m tdf\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe 6 Countries\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    401\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "set_css()\n",
    "con_list = []\n",
    "con_dict = {}\n",
    "t_list = []\n",
    "t_df = pd.DataFrame()\n",
    "for dirname, _, filenames in os.walk(dir):\n",
    "    print(\"................................\")\n",
    "    for paths in filenames:\n",
    "        if paths.lower().endswith(\".csv\"):\n",
    "          con_list.append(paths[0:2])\n",
    "          x = load_data(paths)\n",
    "          t_list.append(x)\n",
    "          x = x.drop(columns=['video_id','region'],axis=1)\n",
    "          globals()[str(paths[0:2])] = x\n",
    "          con_dict[str(paths[0:2])]=x\n",
    "          print(\"------Created\",paths[0:2],\"DataFrame------\")\n",
    "    print(\"Processed all available Datasets\")\n",
    "t_df = pd.concat(t_list,ignore_index=True)\n",
    "tdf = t_df\n",
    "tdf.name = \"The 6 Countries\"\n",
    "print(\"................................\")\n",
    "print(\"tdf  is the cumulative DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EauET5Zs2jGU"
   },
   "outputs": [],
   "source": [
    "def get_unique_categ(data):\n",
    "  cat_x = data['category'].unique()\n",
    "  cat_y = list(cat_x)\n",
    "  if \"Unknown\" in cat_y:\n",
    "    cat_y.remove('Unknown')\n",
    "  dictc = {}\n",
    "  for i in range(0,len(cat_y)):\n",
    "    dictc[i] = cat_y[i]\n",
    "  return dictc,cat_x\n",
    "# dict_cat,cat_x = get_unique_categ(tdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LF449RyLeq_H"
   },
   "source": [
    "##Filling the Missing Categories using Word Vectorisers and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "7qFRrQoCdfv6",
    "outputId": "e13b813c-322b-4850-de0e-fb3aeddea2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA 74\n",
      "DE 256\n",
      "FR 114\n",
      "GB 90\n",
      "IN 103\n"
     ]
    }
   ],
   "source": [
    "for i,j in con_dict.items():\n",
    "  print(i,j[j.category==\"Unknown\"].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZ7B9EhJmnzV"
   },
   "outputs": [],
   "source": [
    "gnb_model = MultinomialNB()\n",
    "rfc_model= RandomForestClassifier(n_estimators=20,verbose=False,n_jobs=-1)\n",
    "dtc_model = DecisionTreeClassifier()\n",
    "xgb_model=XGBClassifier()\n",
    "knn_model = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oSRq7VufgKs"
   },
   "outputs": [],
   "source": [
    "def create_model_data(data,nam):\n",
    "  mdf = data[[\"title\",\"category\"]]\n",
    "  mdf = mdf[mdf['category']!=\"Unknown\"].reset_index()\n",
    "  mdf['cid'] = 0\n",
    "  dict_a,_ = get_unique_categ(data)\n",
    "  for index,i in enumerate(mdf.category):\n",
    "    for j in dict_a.keys():\n",
    "      if dict_a[j]==i:\n",
    "        mdf.at[index,'cid'] = j\n",
    "  v = CountVectorizer()\n",
    "  words = v.fit_transform(mdf['title'].values)\n",
    "  cats = mdf.cid.values\n",
    "  filename = nam + '_vector.pkl'\n",
    "  pickle.dump(v, open(filename, 'wb'))\n",
    "  return words,cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_K6tIBnpOpF"
   },
   "outputs": [],
   "source": [
    "# def find_best_knn(X_train,y_train):\n",
    "#   from sklearn.model_selection import GridSearchCV\n",
    "#   knn2 = KNeighborsClassifier()\n",
    "#   param_grid = {'n_neighbors': np.arange(1,8)}\n",
    "#   knn_gscv = GridSearchCV(knn2, param_grid, cv=5)\n",
    "#   knn_gscv.fit(X_train, y_train)\n",
    "#   print(\"Optimal value for neighbors is->\",knn_gscv.best_params_,\"With accuracy->\",knn_gscv.best_score_)\n",
    "#   return list(knn_gscv.best_params_.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWm2mdm4xG7M"
   },
   "outputs": [],
   "source": [
    "def create_ensemble(data,nam):\n",
    "  print(\"-----------------Starting for\",nam,\"-----------------\")\n",
    "  X,y = create_model_data(data,nam)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2)\n",
    "  dtc = dtc_model.fit(X,y)\n",
    "  gnb = gnb_model.fit(X,y)\n",
    "  xgb = xgb_model.fit(X,y)\n",
    "  rfc = rfc_model.fit(X,y)\n",
    "\n",
    "  knn = knn_model.fit(X_train,y_train)\n",
    "  print(\"----------------------------------------\",\"Implementing Ensemble model has started\",\"----------------------------------------\")\n",
    "  estimators=[('gnb_model', gnb), ('xgb_model', xgb), ('dtc_model',dtc), ('knn_model', knn), ('rfc_model', rfc)]\n",
    "  # ens_name = \"ensemble_\" + str(nam)\n",
    "  ens_name = VotingClassifier(estimators, voting='hard')\n",
    "  ens_name.fit(X,y)\n",
    "  acc_ens = ens_name.score(X_test, y_test)\n",
    "  print(\"----------------------------------------\",\"Implementing Ensemble model has finished\",\"----------------------------------------\")\n",
    "  print(\"\\nThe Ensemble Model for %s has achieved an accuracy of %.4f\" %(nam,acc_ens))\n",
    "  filename = nam + '_model.pkl'\n",
    "  pickle.dump(ens_name, open(filename, 'wb'))\n",
    "  return ens_name  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "MecUqA4erg0m",
    "outputId": "2a46af49-730b-49ea-a833-cda004f0f86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Starting for CA -----------------\n",
      "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
      "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
      "\n",
      "The Ensemble Model for CA has achieved an accuracy of 0.9977\n",
      "-----------------Starting for DE -----------------\n",
      "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
      "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
      "\n",
      "The Ensemble Model for DE has achieved an accuracy of 0.9978\n",
      "-----------------Starting for FR -----------------\n",
      "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
      "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
      "\n",
      "The Ensemble Model for FR has achieved an accuracy of 0.9963\n",
      "-----------------Starting for GB -----------------\n",
      "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
      "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
      "\n",
      "The Ensemble Model for GB has achieved an accuracy of 0.9990\n",
      "-----------------Starting for IN -----------------\n",
      "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
      "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
      "\n",
      "The Ensemble Model for IN has achieved an accuracy of 0.9980\n"
     ]
    }
   ],
   "source": [
    "def perform_ensemble():\n",
    "  for i,j in con_dict.items():\n",
    "    if j[j.category==\"Unknown\"].value_counts().sum() > 0:\n",
    "      globals()[str(\"ensemble_\") + str(i)]  = create_ensemble(j,i)\n",
    "perform_ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1Y_ytZWio4R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DA_category_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
